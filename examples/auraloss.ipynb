{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "auraloss.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bl3voWCkk3h"
      },
      "source": [
        "!wget https://zenodo.org/record/3824876/files/SignalTrain_LA2A_Dataset_1.1.tgz?download=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ngnQna5wVUd"
      },
      "source": [
        "!tar -xvf SignalTrain_LA2A_Dataset_1.1.tgz?download=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oTslF_uMi7j"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "020lsO59kn0X"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf2KwNnqNOzK"
      },
      "source": [
        "!mv SignalTrain_LA2A_Dataset_1.1/ \"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7wFbvG3TWjU"
      },
      "source": [
        "!mv ssh.tar.gz \"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_KkMs1qTi_i"
      },
      "source": [
        "!rm -rf /root/.ssh\n",
        "!mkdir /root/.ssh\n",
        "!tar -xvzf \"/content/drive/My Drive/ssh.tar.gz\"\n",
        "!cp ssh-colab/* /root/.ssh && rm -rf ssh-colab && rm -rf ssh.tar.gz\n",
        "#!chmod 700 /root/.ssh$\n",
        "!touch /root/.ssh/known_hosts\n",
        "!ssh-keyscan github.com >> /root/.ssh/known_hosts\n",
        "!chmod 644 /root/.ssh/known_hosts\n",
        "!chmod 600 /root/.ssh/id_rsa_colab\n",
        "!ssh -T git@github.com"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeHvtEXRSV5d"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N88_6TctYA8G"
      },
      "source": [
        "!pip install git+ssh://git@github.com/csteinmetz1/auraloss.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGRzWf1PUs74"
      },
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNHKahKCSWAy"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import torch\n",
        "import auraloss # here is our package!\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import torchsummary\n",
        "from google.colab import output\n",
        "import pytorch_lightning as pl\n",
        "from argparse import ArgumentParser\n",
        "torchaudio.set_audio_backend(\"sox_io\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StL1vcYmRnr2"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhMSoTGkCBpq"
      },
      "source": [
        "# first we will load the full dataset onto the local disk (this takes about 20 min)\n",
        "!mkdir \"/content/data\"\n",
        "!rsync -aP \"/content/drive/My Drive/SignalTrain_LA2A_Dataset_1.1.zip/\" \"/content/data/\"\n",
        "!unzip \"/content/data/SignalTrain_LA2A_Dataset_1.1.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4jNI_eRM--R"
      },
      "source": [
        "class SignalTrainLA2ADataset(torch.utils.data.Dataset):\n",
        "    \"\"\" SignalTrain LA2A dataset. Source: [10.5281/zenodo.3824876](https://zenodo.org/record/3824876).\"\"\"\n",
        "    def __init__(self, root_dir, subset=\"train\", length=16384, preload=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Path to the root directory of the SignalTrain dataset.\n",
        "            subset (str, optional): Pull data either from \"train\", \"val\", or \"test\" subsets. (Default: \"train\")\n",
        "            length (int, optional): Number of samples in the returned examples. (Default: 40)\n",
        "            preload (bool, optional): Read in all data into RAM during init. (Default: False)\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.subset = subset\n",
        "        self.length = length\n",
        "        self.preload = preload\n",
        "\n",
        "        # get all the target files files in the directory first\n",
        "        self.target_files = glob.glob(os.path.join(self.root_dir, self.subset.capitalize(), \"target_*.wav\"))\n",
        "        self.input_files  = glob.glob(os.path.join(self.root_dir, self.subset.capitalize(), \"input_*.wav\"))\n",
        "        self.params       = [(float(f.split(\"__\")[1].replace(\".wav\",\"\")), float(f.split(\"__\")[2].replace(\".wav\",\"\"))) for f in self.target_files]\n",
        "\n",
        "        self.examples = [] \n",
        "        self.audio_files = []\n",
        "        self.hours = 0  # total number of hours of data in the subset\n",
        "\n",
        "        # ensure that the sets are ordered correctlty\n",
        "        self.target_files.sort()\n",
        "        self.input_files.sort()\n",
        "\n",
        "        # loop over files to count total length\n",
        "        for idx, (tfile, ifile, params) in enumerate(zip(self.target_files, self.input_files, self.params)):\n",
        "            print(os.path.basename(tfile), os.path.basename(ifile))\n",
        "            md = torchaudio.info(tfile)\n",
        "            self.hours += (md.num_frames / md.sample_rate) / 3600 \n",
        "            num_frames = md.num_frames\n",
        "\n",
        "            if self.preload:\n",
        "              output.clear('status_text')\n",
        "              with output.use_tags('status_text'):\n",
        "                print(f\"* Pre-loading... {idx+1:3d}/{len(self.target_files):3d} ...\")\n",
        "              input, sr  = torchaudio.load(ifile, normalize=False)\n",
        "              target, sr = torchaudio.load(tfile, normalize=False)\n",
        "              #input /= ((2**31) - 1) # apply float32 normalization\n",
        "              #target /= ((2**31) - 1)\n",
        "              input = input.half()\n",
        "              target = target.half()\n",
        "              self.audio_files.append({\"target\" : target, \"input\" : input})\n",
        "              num_frames = input.shape[-1]\n",
        "              if self.subset == \"train\":\n",
        "                if idx > 25: break\n",
        "              if self.subset == \"val\":\n",
        "                if idx > 1: break\n",
        "  \n",
        "            # create one entry for each patch\n",
        "            for n in range((num_frames // self.length) - 1):\n",
        "                offset = int(n * self.length)\n",
        "                self.examples.append({\"idx\": idx, \n",
        "                                      \"target_file\" : tfile, \n",
        "                                      \"input_file\" : ifile, \n",
        "                                      \"params\" : params, \n",
        "                                      \"offset\": offset, \n",
        "                                      \"frames\" : num_frames})\n",
        "\n",
        "        # we then want to get the input files\n",
        "        print(f\"Located {len(self.examples)} examples totaling {self.hours:0.1f} hr in the {self.subset} subset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.preload:\n",
        "          audio_idx = self.examples[idx][\"idx\"]\n",
        "          offset = self.examples[idx][\"offset\"]\n",
        "          input = self.audio_files[audio_idx][\"input\"][:,offset:offset+self.length]\n",
        "          target = self.audio_files[audio_idx][\"target\"][:,offset:offset+self.length]\n",
        "        else:\n",
        "          offset = self.examples[idx][\"offset\"] \n",
        "          input, sr  = torchaudio.load(self.examples[idx][\"input\"], \n",
        "                                      num_frames=self.length, \n",
        "                                       frame_offset=offset, \n",
        "                                       normalize=False)\n",
        "          target, sr = torchaudio.load(self.examples[idx][\"target\"], \n",
        "                                       num_frames=self.length, \n",
        "                                       frame_offset=offset, \n",
        "                                       normalize=False)\n",
        "          # apply float32 normalization\n",
        "          input /= ((2**31) - 1)\n",
        "          target /= ((2**31) - 1)\n",
        "\n",
        "        # at random with p=0.5 flip the phase \n",
        "        if np.random.rand() > 0.5:\n",
        "          input *= -1\n",
        "          target *= -1\n",
        "\n",
        "        # then get the tuple of parameters\n",
        "        params = torch.tensor(self.examples[idx][\"params\"]).unsqueeze(0)\n",
        "        params[:,1] /= 100\n",
        "\n",
        "        return input, target, params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhn_QYoXRsAu"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj9pVigeRrVG"
      },
      "source": [
        "def center_crop(x, shape):\n",
        "    start = (x.shape[-1]-shape[-1])//2\n",
        "    stop  = start + shape[-1]\n",
        "    return x[...,start:stop]\n",
        "\n",
        "class FiLM(torch.nn.Module):\n",
        "    def __init__(self, num_features, cond_dim):\n",
        "        super(FiLM, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.bn = torch.nn.BatchNorm1d(num_features, affine=False)\n",
        "        self.adaptor = torch.nn.Linear(cond_dim, num_features * 2)\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "\n",
        "        cond = self.adaptor(cond)\n",
        "        g, b = torch.chunk(cond, 2, dim=-1)\n",
        "        g = g.permute(0,2,1)\n",
        "        b = b.permute(0,2,1)\n",
        "\n",
        "        x = self.bn(x)      # apply BatchNorm without affine\n",
        "        x = (x * g) + b     # then apply conditional affine\n",
        "\n",
        "        return x\n",
        "\n",
        "class TCNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, padding=0, dilation=1, depthwise=False, conditional=False):\n",
        "        super(TCNBlock, self).__init__()\n",
        "\n",
        "        self.in_ch = in_ch\n",
        "        self.out_ch = out_ch\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.depthwise = depthwise\n",
        "        self.conditional = conditional\n",
        "\n",
        "        groups = out_ch if depthwise and (in_ch % out_ch == 0) else 1\n",
        "\n",
        "        self.conv1 = torch.nn.Conv1d(in_ch, \n",
        "                                     out_ch, \n",
        "                                     kernel_size=kernel_size, \n",
        "                                     padding=padding, \n",
        "                                     dilation=dilation,\n",
        "                                     groups=groups,\n",
        "                                     bias=False)\n",
        "        self.conv2 = torch.nn.Conv1d(out_ch, \n",
        "                                     out_ch, \n",
        "                                     kernel_size=kernel_size, \n",
        "                                     padding=padding, \n",
        "                                     dilation=1,\n",
        "                                     groups=groups,\n",
        "                                     bias=False)\n",
        "\n",
        "        if depthwise:\n",
        "            self.conv1b = torch.nn.Conv1d(out_ch, out_ch, kernel_size=1)\n",
        "            self.conv2b = torch.nn.Conv1d(out_ch, out_ch, kernel_size=1)\n",
        "\n",
        "        self.bn1 = torch.nn.BatchNorm1d(in_ch)\n",
        "\n",
        "        if conditional:\n",
        "            self.film = FiLM(out_ch, 64)\n",
        "        else:\n",
        "            self.bn2 = torch.nn.BatchNorm1d(out_ch)\n",
        "\n",
        "        self.relu1 = torch.nn.LeakyReLU()\n",
        "        self.relu2 = torch.nn.LeakyReLU()\n",
        "\n",
        "        self.res = torch.nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x, p=None):\n",
        "        x_in = x\n",
        "\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        if self.depthwise: # apply pointwise conv\n",
        "            x = self.conv1b(x)\n",
        "\n",
        "        if p is not None:   # apply FiLM conditioning\n",
        "            x = self.film(x, p)\n",
        "        else:\n",
        "            x = self.bn2(x)\n",
        "\n",
        "        x = self.relu2(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        if self.depthwise:\n",
        "            x = self.conv2b(x)\n",
        "\n",
        "        x_res = self.res(x_in)\n",
        "        x = x + center_crop(x_res, x.shape)\n",
        "\n",
        "        return x\n",
        "\n",
        "class TCNModel(pl.LightningModule):\n",
        "    \"\"\" Temporal convolutional network with conditioning module.\n",
        "\n",
        "        Params:\n",
        "            nparams (int): Number of conditioning parameters.\n",
        "            ninputs (int): Number of input channels (mono = 1, stereo 2). Default: 1\n",
        "            ninputs (int): Number of output channels (mono = 1, stereo 2). Default: 1\n",
        "            nblocks (int): Number of total TCN blocks. Default: 10\n",
        "            kernel_size (int): Width of the convolutional kernels. Default: 3\n",
        "            dialation_growth (int): Compute the dilation factor at each block as dilation_growth ** (n % stack_size). Default: 1\n",
        "            channel_growth (int): Compute the output channels at each black as in_ch * channel_growth. Default: 2\n",
        "            channel_width (int): When channel_growth = 1 all blocks use convolutions with this many channels. Default: 64\n",
        "            stack_size (int): Number of blocks that constitute a single stack of blocks. Default: 10\n",
        "            depthwise (bool): Use depthwise-separable convolutions to reduce the total number of parameters. Default: False\n",
        "        \"\"\"\n",
        "    def __init__(self, \n",
        "                 nparams,\n",
        "                 ninputs=1,\n",
        "                 noutputs=1,\n",
        "                 nblocks=10, \n",
        "                 kernel_size=3, \n",
        "                 dilation_growth=1, \n",
        "                 channel_growth=1, \n",
        "                 channel_width=64, \n",
        "                 stack_size=10,\n",
        "                 depthwise=False,\n",
        "                 **kwargs):\n",
        "        super(TCNModel, self).__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # setup loss functions\n",
        "        self.l1      = torch.nn.L1Loss()\n",
        "        self.esr     = auraloss.time.ESRLoss()\n",
        "        self.dc      = auraloss.time.DCLoss()\n",
        "        self.logcosh = auraloss.time.LogCoshLoss()\n",
        "        self.stft    = auraloss.freq.STFTLoss()\n",
        "        self.mrstft  = auraloss.freq.MultiResolutionSTFTLoss()\n",
        "        self.rrstft  = auraloss.freq.RandomResolutionSTFTLoss()\n",
        "\n",
        "        if nparams > 0:\n",
        "            self.gen = torch.nn.Sequential(\n",
        "                torch.nn.Linear(nparams, 16),\n",
        "                torch.nn.PReLU(),\n",
        "                torch.nn.Linear(16, 32),\n",
        "                torch.nn.PReLU(),\n",
        "                torch.nn.Linear(32, 64),\n",
        "                torch.nn.PReLU()\n",
        "            )\n",
        "\n",
        "        self.blocks = torch.nn.ModuleList()\n",
        "        for n in range(nblocks):\n",
        "            in_ch = out_ch if n > 0 else ninputs\n",
        "            out_ch = in_ch * channel_growth if channel_growth > 1 else channel_width\n",
        "\n",
        "            dilation = dilation_growth ** (n % stack_size)\n",
        "            self.blocks.append(TCNBlock(in_ch, \n",
        "                                        out_ch, \n",
        "                                        kernel_size=kernel_size, \n",
        "                                        dilation=dilation,\n",
        "                                        depthwise=self.hparams.depthwise,\n",
        "                                        conditional=True if nparams > 0 else False))\n",
        "\n",
        "        self.output = torch.nn.Conv1d(out_ch, noutputs, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, p=None):\n",
        "        # if parameters present, \n",
        "        # compute global conditioning\n",
        "        if p is not None:\n",
        "            cond = self.gen(p)\n",
        "        else:\n",
        "            cond = None\n",
        "\n",
        "        # iterate over blocks passing conditioning\n",
        "        for block in self.blocks:\n",
        "            x = block(x, cond)\n",
        "\n",
        "        return self.output(x)\n",
        "\n",
        "    def compute_receptive_field(self):\n",
        "        \"\"\" Compute the receptive field in samples.\"\"\"\n",
        "        rf = self.hparams.kernel_size\n",
        "        for n in range(1,self.hparams.nblocks):\n",
        "            dilation = self.hparams.dilation_growth ** (n % self.hparams.stack_size)\n",
        "            rf = rf + ((self.hparams.kernel_size-1) * dilation)\n",
        "            rf = rf + ((self.hparams.kernel_size-1) * 1)\n",
        "        return rf\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input, target, params = batch\n",
        "\n",
        "        # pass the input thrgouh the mode\n",
        "        pred = self(input, params)\n",
        "\n",
        "        # crop the target signal \n",
        "        target = center_crop(target, pred.shape)\n",
        "\n",
        "        # compute the error using appropriate loss      \n",
        "        if   self.hparams.train_loss == \"l1\":\n",
        "            loss = self.l1(pred, target)\n",
        "        elif self.hparams.train_loss == \"esr+dc\":\n",
        "            loss = self.esr(pred, target) + self.dc(pred, target)\n",
        "        elif self.hparams.train_loss == \"logcosh\":\n",
        "            loss = self.logcosh(pred, target)\n",
        "        elif self.hparams.train_loss == \"stft\":\n",
        "            loss = torch.stack(self.stft(pred, target),dim=0).sum()\n",
        "        elif self.hparams.train_loss == \"mrstft\":\n",
        "            loss = torch.stack(self.mrstft(pred, target),dim=0).sum()\n",
        "        elif self.hparams.train_loss == \"rrstft\":\n",
        "            loss = torch.stack(self.rrstft(pred, target),dim=0).sum()\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Invalid loss fn: {self.hparams.train_loss}\")\n",
        "\n",
        "        self.log('train_loss', \n",
        "                 loss, \n",
        "                 on_step=True, \n",
        "                 on_epoch=True, \n",
        "                 prog_bar=True, \n",
        "                 logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input, target, params = batch\n",
        "\n",
        "        # pass the input thrgouh the mode\n",
        "        pred = self(input, params)\n",
        "\n",
        "        # crop the target signal \n",
        "        target = center_crop(target, pred.shape)\n",
        "\n",
        "        # compute the validation error using all losses\n",
        "        l1_loss      = self.l1(pred, target)\n",
        "        esr_loss     = self.esr(pred, target)\n",
        "        dc_loss      = self.dc(pred, target)\n",
        "        logcosh_loss = self.logcosh(pred, target)\n",
        "        stft_loss    = torch.stack(self.stft(pred, target),dim=0).sum()\n",
        "        mrstft_loss  = torch.stack(self.mrstft(pred, target),dim=0).sum()\n",
        "        rrstft_loss  = torch.stack(self.rrstft(pred, target),dim=0).sum()\n",
        "\n",
        "        aggregate_loss = l1_loss + \\\n",
        "                         esr_loss + \\\n",
        "                         dc_loss + \\\n",
        "                         logcosh_loss + \\\n",
        "                         mrstft_loss + \\\n",
        "                         stft_loss + \\\n",
        "                         rrstft_loss\n",
        "\n",
        "        self.log('val_loss', aggregate_loss)\n",
        "        self.log('val_loss/L1', l1_loss)\n",
        "        self.log('val_loss/ESR', esr_loss)\n",
        "        self.log('val_loss/DC', dc_loss)\n",
        "        self.log('val_loss/LogCosh', logcosh_loss)\n",
        "        self.log('val_loss/STFT', stft_loss)\n",
        "        self.log('val_loss/MRSTFT', mrstft_loss)\n",
        "        self.log('val_loss/RRSTFT', rrstft_loss)\n",
        "\n",
        "        # move tensors to cpu for logging\n",
        "        outputs = {\n",
        "            \"input\" : input.cpu().numpy(),\n",
        "            \"target\": target.cpu().numpy(),\n",
        "            \"pred\"  : pred.cpu().numpy()}\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        # flatten the output validation step dicts to a single dict\n",
        "        outputs = res = {k: v for d in validation_step_outputs for k, v in d.items()} \n",
        "        \n",
        "        i = outputs[\"input\"][0].squeeze()\n",
        "        c = outputs[\"target\"][0].squeeze()\n",
        "        p = outputs[\"pred\"][0].squeeze()\n",
        "\n",
        "        # log audio examples\n",
        "        self.logger.experiment.add_audio(\"input\", i, self.global_step, sample_rate=self.hparams.sample_rate)\n",
        "        self.logger.experiment.add_audio(\"target\", c, self.global_step, sample_rate=self.hparams.sample_rate)\n",
        "        self.logger.experiment.add_audio(\"pred\",   p, self.global_step, sample_rate=self.hparams.sample_rate)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "    # add any model hyperparameters here\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
        "        # --- model related ---\n",
        "        parser.add_argument('--ninputs', type=int, default=1)\n",
        "        parser.add_argument('--noutputs', type=int, default=1)\n",
        "        parser.add_argument('--nblocks', type=int, default=10)\n",
        "        parser.add_argument('--kernel_size', type=int, default=3)\n",
        "        parser.add_argument('--dilation_growth', type=int, default=1)\n",
        "        parser.add_argument('--channel_growth', type=int, default=1)\n",
        "        parser.add_argument('--channel_width', type=int, default=64)\n",
        "        parser.add_argument('--stack_size', type=int, default=10)\n",
        "        parser.add_argument('--depthwise', default=False, action='store_true')\n",
        "        # --- training related ---\n",
        "        parser.add_argument('--lr', type=float, default=1e-3)\n",
        "        parser.add_argument('--train_loss', type=str, default=\"l1\")\n",
        "\n",
        "        return parser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-QC1cztRy06"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PBu7O7_R0fp"
      },
      "source": [
        "# add PROGRAM level args\n",
        "#root_dir = '/content/drive/My Drive/SignalTrain_LA2A_Dataset_1.1'\n",
        "root_dir = '/content/data/SignalTrain_LA2A_Dataset_1.1'\n",
        "sample_rate = 44100\n",
        "shuffle = True\n",
        "train_subset = \"train\"\n",
        "val_subset = \"val\"\n",
        "train_length = 16384\n",
        "eval_length = 262144\n",
        "batch_size = 128\n",
        "num_workers = 0\n",
        "preload = False\n",
        "precision = 16\n",
        "\n",
        "# init the trainer and model \n",
        "trainer = pl.Trainer(gpus=1, precision=precision)\n",
        "\n",
        "# setup the dataloaders\n",
        "train_dataset = SignalTrainLA2ADataset(root_dir, \n",
        "                                      subset=train_subset,\n",
        "                                      length=train_length,\n",
        "                                      preload=preload)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                               shuffle=shuffle,\n",
        "                                               batch_size=batch_size,\n",
        "                                               num_workers=num_workers)\n",
        "\n",
        "val_dataset = SignalTrainLA2ADataset(root_dir, \n",
        "                                    subset=val_subset,\n",
        "                                    length=eval_length,\n",
        "                                    preload=preload)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                             shuffle=shuffle,\n",
        "                                             batch_size=batch_size,\n",
        "                                             num_workers=num_workers)\n",
        "\n",
        "dict_args = {\n",
        "      \"nparams\" : 2,\n",
        "      \"ninputs\" : 1,\n",
        "      \"noutputs\" : 1,\n",
        "      \"nblocks\" : 10, \n",
        "      \"kernel_size\": 3, \n",
        "      \"dilation_growth\" : 1, \n",
        "      \"channel_growth\" : 1, \n",
        "      \"channel_width\" : 64, \n",
        "      \"stack_size\" : 10,\n",
        "      \"depthwise\" : False,\n",
        "      \"lr\" : 0.001,\n",
        "      \"sample_rate\" : sample_rate,\n",
        "      \"train_loss\" : \"mrstft\"\n",
        "}\n",
        "model = TCNModel(**dict_args)\n",
        "\n",
        "# find proper learning rate\n",
        "#trainer.tune(model, train_dataloader)\n",
        "\n",
        "#torchsummary.summary(model, [(1,eval_length), (1,2)])\n",
        "\n",
        "device = \"cuda:0\"\n",
        "model.stft.to(device)\n",
        "\n",
        "# train!\n",
        "trainer.fit(model, train_dataloader, val_dataloader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF-sjlZbbALa"
      },
      "source": [
        " # Start tensorboard.\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sp_005lpYl3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}